{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章: ベクトル空間法(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 80. コーパスの整形"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1/10サンプリングデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"enwiki-20150112-400-r10-105752.txt\") as fr, \\\n",
    "      open(\"EWikiCorpus.txt\", mode=\"w\") as fw:\n",
    "    for line in fr:\n",
    "        tokens = []\n",
    "        line = line.rstrip()\n",
    "        for chunk in line.split():\n",
    "            token = chunk.strip().strip('.,!?;:()[]\\'\"')\n",
    "            if len(token) > 0:\n",
    "                tokens.append(token)\n",
    "        print(*tokens, sep=\" \", end=\"\\n\", file=fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1/100サンプリングデータ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"enwiki-20150112-400-r100-10576.txt\") as fr, \\\n",
    "      open(\"EWikiCorpusMini.txt\", mode=\"w\") as fw:\n",
    "    for line in fr:\n",
    "        tokens = []\n",
    "        line = line.rstrip()\n",
    "        for chunk in line.split():\n",
    "            token = chunk.strip().strip('.,!?;:()[]\\'\"')\n",
    "            if len(token) > 0:\n",
    "                tokens.append(token)\n",
    "        print(*tokens, sep=\" \", end=\"\\n\", file=fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 81. 複合語からなる国名への対処"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webスクレイピングでネットから世界の国名リストをもらってくる\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "target_url = \"https://www.nationsonline.org/oneworld/countries_of_the_world.htm\"\n",
    "r = requests.get(target_url)\n",
    "soup = BeautifulSoup(r.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#取得したスープから国のリストを生成\n",
    "import re\n",
    "country_list = []\n",
    "for tr in soup.find_all(\"tr\"):\n",
    "    country = []\n",
    "    for td in tr.find_all(\"td\", class_=(\"tdb\", \"tdx\")):\n",
    "        country.append(td.string)\n",
    "    if len(country) > 0:\n",
    "        country_name = str(country[0]).split()\n",
    "        if len(country_name) > 1:\n",
    "            country_list.append(\" \".join(country_name))\n",
    "            \n",
    "country_list.remove(\"Cocos (Keeling) Islands\")\n",
    "country_list.remove(\"Iran (Islamic Republic of)\")\n",
    "country_list.remove(\"Korea, Democratic People's Rep. (North Korea)\")\n",
    "country_list.remove(\"Korea, Republic of (South Korea)\")\n",
    "country_list.remove(\"Lao, People's Democratic Republic\")\n",
    "country_list.remove(\"Macedonia, Rep. of\")\n",
    "country_list.remove('Micronesia, Federal States of')\n",
    "country_list.remove(\"Moldova, Republic of\")\n",
    "country_list.remove(\"Slovakia (Slovak Republic)\")\n",
    "country_list.remove('Vatican City State (Holy See)')\n",
    "country_list.remove(\"Virgin Islands (British)\")\n",
    "country_list.remove(\"Virgin Islands (U.S.)\")\n",
    "country_list.append(\"Cocos Islands\")\n",
    "country_list.append(\"North Korea\")\n",
    "country_list.append(\"South Korea\")\n",
    "country_list.append(\"Slovak Republic\")\n",
    "country_list.append(\"Vatican City State\")\n",
    "country_list.append(\"Virgin Islands\")\n",
    "country_list.append(\"Isle of Man\")\n",
    "\n",
    "with open(\"CountryList.txt\", mode=\"w\") as fw:\n",
    "    for country in country_list:\n",
    "        print(country, file=fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'American': 'American Samoa', 'Antigua': 'Antigua and Barbuda', 'Bosnia': 'Bosnia and Herzegovina', 'Brunei': 'Brunei Darussalam', 'Burkina': 'Burkina Faso', 'Cape': 'Cape Verde', 'Cayman': 'Cayman Islands', 'Central': 'Central African Republic', 'Christmas': 'Christmas Island', 'Cook': 'Cook Islands', 'Costa': 'Costa Rica', 'Czech': 'Czech Republic', 'El': 'El Salvador', 'Equatorial': 'Equatorial Guinea', 'Falkland': 'Falkland Islands', 'Faroe': 'Faroe Islands', 'French': 'French Southern Territories', 'Great': 'Great Britain', 'Holy': 'Holy See', 'Hong': 'Hong Kong', 'Ivory': 'Ivory Coast', 'Marshall': 'Marshall Islands', 'Netherlands': 'Netherlands Antilles', 'New': 'New Zealand', 'Northern': 'Northern Mariana Islands', 'Palestinian': 'Palestinian territories', 'Papua': 'Papua New Guinea', 'Pitcairn': 'Pitcairn Island', 'Puerto': 'Puerto Rico', 'Reunion': 'Reunion Island', 'Russian': 'Russian Federation', 'Saint': 'Saint Vincent and the Grenadines', 'San': 'San Marino', 'Sao': 'Sao Tome and Principe', 'Saudi': 'Saudi Arabia', 'Sierra': 'Sierra Leone', 'Solomon': 'Solomon Islands', 'South': 'South Korea', 'Sri': 'Sri Lanka', 'Trinidad': 'Trinidad and Tobago', 'Turks': 'Turks and Caicos Islands', 'United': 'United States', 'Wallis': 'Wallis and Futuna Islands', 'Western': 'Western Sahara', 'Cocos': 'Cocos Islands', 'North': 'North Korea', 'Slovak': 'Slovak Republic', 'Vatican': 'Vatican City State', 'Virgin': 'Virgin Islands', 'Isle': 'Isle of Man'}\n"
     ]
    }
   ],
   "source": [
    "country_dict = {country.split()[0]: country for country in country_list}\n",
    "print(country_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"EWikiCorpusMini.txt\") as fr, \\\n",
    "      open(\"EMiniMod.txt\", mode=\"w\") as fw:\n",
    "    for line in fr:\n",
    "        line = line.rstrip()\n",
    "        line = line.split()\n",
    "        for wordindex, oneword in enumerate(line):\n",
    "            if oneword in country_dict.keys():\n",
    "                if \" \".join(line[wordindex:(wordindex+len(country_dict[oneword])-1)]) == country_dict[oneword]:\n",
    "                    new_word = \"_\".join(line[wordindex:(wordindex+len(country_dict[oneword])-1)])\n",
    "                    del line[wordindex:(wordindex+len(country_dict[oneword])-1)]\n",
    "                    line.insert(wordindex, new_word)\n",
    "        print(\" \".join(line), file=fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 82. 文脈の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with open(\"EMiniMod.txt\") as fr, \\\n",
    "      open(\"EMiniContext.txt\", mode=\"w\") as fw:\n",
    "    for line in fr:\n",
    "        line = line.rstrip()\n",
    "        line = line.split()\n",
    "        for t_index in range(len(line)):\n",
    "            d = random.randint(1, 5)\n",
    "            for i in range(max(t_index-d, 0), min(t_index+d+1, len(line))):\n",
    "                if i != t_index:\n",
    "                    print(line[t_index] + \"\\t\" + line[i], file=fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 83. 単語/文脈の頻度の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 68153346\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "with open(\"EMiniContext.txt\") as fr:\n",
    "    list_tc = []\n",
    "    list_t = []\n",
    "    list_c = []\n",
    "    N = 0\n",
    "    for line in fr:\n",
    "        line = line.rstrip()\n",
    "        tokens = line.split(\"\\t\")\n",
    "        list_tc.append(line)\n",
    "        list_t.append(tokens[0])\n",
    "        list_c.append(tokens[1])\n",
    "        N += 1\n",
    "\n",
    "N -= 1\n",
    "print(\"N: {}\".format(N))\n",
    "Count_tc = Counter(list_tc)\n",
    "Count_t = Counter(list_t)\n",
    "Count_c = Counter(list_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'N' (int)\n",
      "Stored 'Count_tc' (Counter)\n",
      "Stored 'Count_t' (Counter)\n",
      "Stored 'Count_c' (Counter)\n"
     ]
    }
   ],
   "source": [
    "%store N\n",
    "%store Count_tc\n",
    "%store Count_t\n",
    "%store Count_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 84. 単語文脈行列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r N\n",
    "%store -r Count_tc\n",
    "%store -r Count_t\n",
    "%store -r Count_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix\n",
    "from collections import OrderedDict\n",
    "\n",
    "t_index = OrderedDict((key, i) for i, key in enumerate(Count_t.keys()))\n",
    "c_index = OrderedDict((key, i) for i, key in enumerate(Count_c.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = lil_matrix((len(Count_t), len(Count_c)))\n",
    "\n",
    "for key in Count_tc.keys():\n",
    "    if Count_tc[key] >= 10:\n",
    "        t, c = key.split(\"\\t\")\n",
    "        ppmi = max(np.log((N*Count_tc[key]) / (Count_t[t]*Count_c[c])), 0)\n",
    "        X[t_index[t], c_index[c]] = ppmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X' (lil_matrix)\n"
     ]
    }
   ],
   "source": [
    "%store X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 85. 主成分分析による次元圧縮"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original X shape: (383307, 383307)\n",
      "X_pca shape:(383307, 300)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "pca = TruncatedSVD(n_components = 300)\n",
    "X_pca = pca.fit_transform(X)\n",
    "print(\"original X shape: {}\".format(X.shape))\n",
    "print(\"X_pca shape:{}\".format(X_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'X_pca' (ndarray)\n"
     ]
    }
   ],
   "source": [
    "%store X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 86. 単語ベクトルの表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.79199585e-01 -7.92988847e-02 -5.30843002e-01  3.57516690e-01\n",
      " -6.41936835e-02  3.97280914e-01  1.63381849e-02 -2.80473779e-01\n",
      " -5.57055979e-02 -2.48698978e-01 -7.03300722e-01  6.91704965e-02\n",
      " -5.17045439e-01 -7.72291770e-02 -6.75260409e-02 -2.86649372e-01\n",
      "  2.45289175e-01  4.29554749e-02 -1.14095646e-01 -3.28966262e-01\n",
      "  3.19320944e-01 -2.59354859e-01  4.87557649e-01  1.94632276e-01\n",
      " -1.77250228e-01  5.77243665e-02  2.24807974e-02 -5.86683600e-01\n",
      "  2.11993848e-01 -2.62488494e-01 -1.68962551e-01 -2.74591687e-01\n",
      "  2.55959399e-02 -9.37128017e-02  1.86827791e-01  1.95056082e-01\n",
      "  6.75651087e-02  1.18463287e-01 -2.28338566e-02  2.11160935e-02\n",
      "  1.25455810e-01  3.21649831e-01 -2.46326468e-01 -2.22650021e-01\n",
      " -3.45318267e-02 -1.68043816e-01  1.52377445e-01  3.72811445e-01\n",
      "  1.46397917e-01  8.92022354e-02  1.18447301e-01 -3.05462224e-01\n",
      "  5.76099460e-02  9.34595654e-02  2.50573183e-01  6.22760763e-01\n",
      " -4.62849194e-01  4.95562402e-04 -6.38552515e-02 -7.68523887e-01\n",
      "  2.08025213e-01 -8.10863467e-03  1.29817484e-01 -4.57604578e-02\n",
      "  2.70768271e-01  9.02078719e-02  1.61361139e-01  7.89872533e-02\n",
      " -1.70721471e-01 -3.29158959e-01 -1.02347038e-01 -3.62819035e-02\n",
      "  1.60955596e-02  1.31739326e-01 -8.44826313e-02 -3.78081315e-02\n",
      " -8.11639919e-02 -1.24154082e-01 -1.50208036e-01  7.79426923e-02\n",
      "  1.99428771e-03 -2.86922011e-02  2.74020985e-01  1.56341731e-01\n",
      " -2.28246688e-01 -7.57212477e-02 -8.12167035e-03  1.14584286e-01\n",
      " -1.27385305e-01 -1.10500603e-01  1.17743332e-01 -1.21709541e-02\n",
      "  1.07124411e-03  2.54822224e-02  1.04473596e-01 -2.81872640e-02\n",
      " -7.70589714e-02 -1.55960240e-01  3.53344923e-02 -5.83255759e-02\n",
      "  1.96139346e-02  4.07920029e-03  6.13522203e-02  2.86427785e-02\n",
      "  1.52570919e-01 -2.64346268e-02 -1.16570213e-01  7.89434364e-03\n",
      " -6.79966222e-02  4.20958951e-02 -1.19444011e-02  9.98623092e-02\n",
      " -5.22669321e-02 -1.92150043e-02 -1.95403469e-01 -6.15975313e-02\n",
      "  5.32456689e-02 -3.54067158e-02 -1.52524389e-02 -3.47904395e-02\n",
      "  7.60958936e-02 -6.01810451e-05 -6.54807456e-02 -4.59413620e-02\n",
      "  4.55119421e-02  1.29966527e-02  3.59256554e-02  4.51070866e-02\n",
      " -5.57789894e-02  5.30039494e-03  1.05383705e-01  1.66617073e-02\n",
      "  4.42616343e-03 -9.25237066e-03  4.82637095e-03  4.16766395e-02\n",
      "  2.37150216e-02  2.87649067e-02 -7.21395920e-02 -4.11968343e-02\n",
      " -1.14954677e-01 -2.57365124e-02 -9.45292681e-03  7.40192254e-03\n",
      "  2.55230144e-02 -1.09045454e-01 -4.57955869e-02  9.84047804e-03\n",
      "  5.56842996e-02  9.56664791e-02 -2.33461742e-02 -2.92314202e-02\n",
      " -2.14076323e-02 -7.30773677e-02  4.43530599e-02  5.69686715e-02\n",
      "  7.25666271e-02  1.63425558e-02  3.31530250e-02 -2.14008634e-03\n",
      "  2.95258166e-02  6.68528080e-02 -5.98983095e-02 -4.48165342e-02\n",
      "  1.28206079e-02 -6.57152789e-02 -2.94963170e-02 -1.93945685e-03\n",
      "  3.73392930e-02 -4.04665075e-02 -6.23185900e-04  6.79482032e-02\n",
      "  2.29429232e-02  2.25618227e-02 -2.08970425e-02 -5.64480375e-02\n",
      " -5.06966215e-02  1.68896510e-03  3.18626672e-02 -2.75351423e-02\n",
      " -1.46195679e-02  1.00618264e-02  3.04504340e-02 -3.63546651e-02\n",
      "  2.21035149e-02 -2.76132198e-02 -6.17753508e-02 -1.62355826e-02\n",
      " -5.99780664e-02  2.49528511e-02 -4.83581407e-02  2.27434289e-02\n",
      "  4.95507252e-02 -1.52234179e-04 -2.51546231e-02  5.07575843e-02\n",
      "  2.19495146e-02 -1.11633605e-02  5.25826713e-03 -4.53680976e-02\n",
      "  8.76211000e-03  2.30304316e-02  8.14462856e-02  4.37640792e-02\n",
      "  7.26121400e-02  1.06966531e-02 -2.45423525e-02  4.97377700e-03\n",
      "  2.60061360e-02 -1.03243560e-01  3.31421848e-02  1.93524733e-02\n",
      " -4.79075464e-02  9.97383618e-03  6.14864280e-03  4.51152629e-02\n",
      "  6.27862843e-03 -2.02948500e-02 -7.13225789e-03 -7.66782220e-03\n",
      "  3.14690947e-03  4.40791026e-03 -1.12264595e-02 -8.03563109e-03\n",
      "  1.98286121e-02  9.92010021e-03 -1.26571556e-02 -1.55365357e-02\n",
      "  1.67920271e-02 -5.32741083e-02 -5.33051074e-03  1.48199898e-02\n",
      "  2.38712151e-02 -2.45048640e-02 -4.38368994e-04 -2.18086366e-02\n",
      " -6.83876954e-03 -1.25818660e-02  1.26087866e-02  4.83102753e-02\n",
      " -2.42134150e-02  4.21327991e-02 -4.24628708e-04 -1.34939622e-02\n",
      "  1.18226021e-02 -4.10423384e-02  7.03743916e-03 -2.19408677e-02\n",
      " -6.19007524e-02 -4.93412200e-02  2.06107025e-02  3.14923777e-02\n",
      " -2.10712803e-02  2.30790434e-02  7.28008477e-03  1.71771839e-03\n",
      " -1.13141582e-02 -1.93776901e-02  2.60708758e-02 -9.93715306e-03\n",
      "  1.56398758e-02 -6.58443215e-04  5.80772444e-02  1.06478229e-04\n",
      "  3.36014302e-02  2.11076078e-02  5.89795259e-04 -3.55714764e-03\n",
      "  4.56691192e-03  1.11938933e-02 -1.75238472e-03 -1.15415056e-02\n",
      "  1.46502887e-03  7.72922869e-03  6.97881670e-03  2.70195757e-02\n",
      " -7.09131325e-03  1.67904116e-03  2.06187809e-02  5.93399381e-03\n",
      "  1.15208510e-02  1.17546309e-02 -3.21885207e-02  9.70299595e-05\n",
      " -1.49285281e-02  2.86452291e-03  1.66494929e-02 -5.02601235e-03\n",
      " -3.28261246e-03 -1.32169488e-02  1.22215255e-02 -1.11047130e-02\n",
      "  2.90048575e-02  4.04562198e-03  7.45415367e-03 -1.88732100e-02\n",
      "  2.37848914e-02 -1.52429247e-02  7.58100633e-03  9.33106515e-03]\n"
     ]
    }
   ],
   "source": [
    "print(X_pca[t_index[\"United_States\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 87. 単語の類似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03324777703015864\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "print(cos_sim(X_pca[t_index[\"United_States\"]], X_pca[t_index[\"U.S\"]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 88. 類似度の高い単語10件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takapen325/.pyenv/versions/anaconda3-5.2.0/envs/mainEnv/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Vec_england = X_pca[t_index[\"England\"]]\n",
    "\n",
    "list_Vec = []\n",
    "for index in t_index.values():\n",
    "    temp_sim = cos_sim(Vec_england, X_pca[index])\n",
    "    list_Vec.append(temp_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Framework\t-0.09834245111896592\n",
      "Asset\t-0.09505085734428634\n",
      "Ivor\t-0.09445805971862348\n",
      "Ater\t-0.09445805971862346\n",
      "Horthy\t-0.0917532226551083\n",
      "Muak\t-0.09155921802754437\n",
      "Duran\t-0.08872148434800202\n",
      "Dadullah\t-0.0863716311506307\n",
      "Roi\t-0.0862304870031103\n",
      "Ching\t-0.08623027502483759\n"
     ]
    }
   ],
   "source": [
    "Sim_rank = np.argsort(list_Vec)[::1]\n",
    "list_word = list(t_index.keys())\n",
    "for index in Sim_rank[:10]:\n",
    "    print(list_word[index] + \"\\t\" + str(list_Vec[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 89. 加法構成性によるアナロジー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takapen325/.pyenv/versions/anaconda3-5.2.0/envs/mainEnv/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bois\t-0.14715548511358012\n",
      "Mundial\t-0.14456570550355424\n",
      "Nacional\t-0.14317157626344854\n",
      "Pole\t-0.14301961026977175\n",
      "España\t-0.14280670370012225\n",
      "Varzim\t-0.14280670370012225\n",
      "Rambaud\t-0.14280670370012225\n",
      "comte\t-0.14280670370012225\n",
      "Vaca\t-0.14280670370012225\n",
      "los\t-0.14280670370012225\n"
     ]
    }
   ],
   "source": [
    "Vec_question = X_pca[t_index[\"Spain\"]] - X_pca[t_index[\"Madrid\"]] + X_pca[t_index[\"Athens\"]]\n",
    "\n",
    "list_Vec2 = []\n",
    "for index in t_index.values():\n",
    "    temp_sim = cos_sim(Vec_question, X_pca[index])\n",
    "    list_Vec2.append(temp_sim)\n",
    "    \n",
    "Sim_rank2 = np.argsort(list_Vec2)[::1]\n",
    "list_word = list(t_index.keys())\n",
    "for index in Sim_rank2[:10]:\n",
    "    print(list_word[index] + \"\\t\" + str(list_Vec2[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainEnv",
   "language": "python",
   "name": "mainenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
